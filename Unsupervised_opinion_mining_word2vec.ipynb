{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkNhbEXD8qqj",
        "outputId": "dd2c3894-57d4-443b-b7fd-e14719964da0"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec,KeyedVectors\n",
        "from gensim.test.utils import datapath\n",
        "import re\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import gensim\n",
        "import multiprocessing\n",
        "import random\n",
        "nltk.download('stopwords')\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4hC0zEa-0N2"
      },
      "source": [
        "#Clean the data function\n",
        "stopwords_list = stopwords.words('english')\n",
        "cores=multiprocessing.cpu_count()\n",
        "import string\n",
        "\n",
        "def clean_data(w):\n",
        "    w = w.lower()                                                                             #Lower casing\n",
        "    w=re.sub(r'[^\\w\\s]','',w)                                                                 #Remove other special characters\n",
        "    w=re.sub(r\"([0-9])\", r\" \",w)                                                              #Remove numbers       \n",
        "    words = w.split() \n",
        "    clean_words = [word for word in words if (word not in stopwords_list) and len(word) > 2]  #Remove stop and short words\n",
        "    return \" \".join(clean_words)  \n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-GHsIEQ_HDI",
        "outputId": "8ee4085d-45bf-42b8-91c1-3f4fa2141f6b"
      },
      "source": [
        "# Please, always put this cell on top of any Notebook you make.\n",
        "# It replaces the former cells with Google Drive mounting and change directory.\n",
        "# Using the code in this cell ensures that the Notebook is easy to run on Google Colab and on Jupyter Lab alike.\n",
        "import os\n",
        "\n",
        "def init_google_colab(gpath = '/content/gdrive/MyDrive/Assignment-Semester-7/NLP/Assignment-3'):\n",
        "    \"\"\"If in Google Colab, switch to relevant working directory\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    bool    True if running in Google Colab, False if not.\n",
        "    \"\"\"\n",
        "    in_google_colab = False\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        in_google_colab = True\n",
        "    except:\n",
        "        pass\n",
        "    if in_google_colab:\n",
        "        drive.mount(\"/content/gdrive\")\n",
        "        os.chdir(gpath)\n",
        "    return in_google_colab\n",
        "\n",
        "init_google_colab('/content/gdrive/MyDrive/Assignment-Semester-7/NLP/Assignment-3')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E969VLzsauFD"
      },
      "source": [
        "def punc(text):\n",
        "  exclude = set(string.punctuation)\n",
        "  text = [''.join(x for x in y if x not in exclude) for y in text]\n",
        "  text = [x for x in text if x not in stopwords.words('english')]\n",
        "  return text\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WwqG90l_kwi"
      },
      "source": [
        "from pandas import read_excel\n",
        "from collections import Counter\n",
        "data = read_excel(\"assignment3_data.xlsx\")\n",
        "data.columns = [\"Data\"]\n",
        "\n",
        "#Extracting the words\n",
        "\n",
        "l1 = []\n",
        "for index, row in data.iterrows():\n",
        "    words = row[\"Data\"].split(\" \")\n",
        "    for j in words:\n",
        "        l1.append(j.lower())\n",
        "\n",
        "#Removing punctuations and stopwords\n",
        "        \n",
        "def punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    text = [''.join(x for x in y if x not in exclude) for y in text]\n",
        "    text = [x for x in text if x not in stopwords.words('english')]\n",
        "    return text\n",
        "\n",
        "l1 = punc(l1)\n",
        "\n",
        "POS = ['JJ','JJR','JJS','NN','NNS','NNP','NNPS']\n",
        "temp_vocab = sorted([word for (word,pos) in nltk.pos_tag(l1) if pos in POS])\n",
        "df1 = pd.DataFrame(columns = [\"Words\"])\n",
        "c1 = Counter(temp_vocab)\n",
        "df1[\"Words\"] = c1.keys()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT2oZ5MXdSEp"
      },
      "source": [
        "df1.to_csv(\"/content/gdrive/MyDrive/Assignment-Semester-7/NLP/Assignment-3/assignment3_data.txt\", index = False)\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/Assignment-Semester-7/NLP/Assignment-3/assignment3_data.txt\",\"r+\") as f:\n",
        "    new_f = f.readlines()\n",
        "    f.seek(0)\n",
        "    for line in new_f:\n",
        "        if \"What qualities do you think are necessary to be the prime minister of India?\" not in line:\n",
        "            f.write(line)\n",
        "    f.truncate()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omsTbyvg-6RA"
      },
      "source": [
        "#Give input\n",
        "with open(\"assignment3_data.txt\") as f:\n",
        "  lines = f.readlines()\n",
        "\n",
        "#lines = clean_data(lines)\n",
        "#model_input=SentGen(lines)\n",
        "processed = [gensim.utils.simple_preprocess(sentence) for sentence in lines]\n",
        "#Initialize the model\n",
        "\n",
        "modelsg = Word2Vec(min_count=1,window=7,size=300,workers=cores-1) \n",
        "#skip-gram: sg=1\n",
        "\n",
        "#Build vocabulary\n",
        "modelsg.build_vocab(processed)     #Building the vocabulary using entire dataset\n",
        "vocab_len=len(modelsg.wv.vocab)\n",
        "\n",
        "#Training the model\n",
        "modelsg.train(processed, total_examples=modelsg.corpus_count, epochs=50)  #Add callbacks, if required\n",
        "\n",
        "#Saving the model\n",
        "modelsg.wv.save_word2vec_format('gensim_w2v_model.bin',binary=True)\n",
        "\n",
        "#Loading the model during testing time\n",
        "trained_model= KeyedVectors.load_word2vec_format('gensim_w2v_model.bin', binary=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqpJXhvpXx8O"
      },
      "source": [
        "import gensim\n",
        "\n",
        "modelg = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Yb2Q7dGX5M7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0390595-18e1-47dd-9553-aa34969d07cb"
      },
      "source": [
        "modelcb = gensim.models.Word2Vec(processed, min_count=1, size = 100, window = 5, sg =0)#cbow\n",
        "\n",
        "modelcb.train(processed, total_examples=modelcb.corpus_count, epochs=50)  #Add callbacks, if required\n",
        "\n",
        "#Saving the model\n",
        "modelcb.wv.save_word2vec_format('gensim_w2v_model1.bin',binary=True)\n",
        "\n",
        "#Loading the model during testing time\n",
        "trained_model1= KeyedVectors.load_word2vec_format('gensim_w2v_model1.bin', binary=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5m97E4iilqC"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "model_01 = KMeans(n_clusters=10, \n",
        "                  max_iter=10000, random_state=None, n_init=100).fit(\n",
        "                      X=trained_model.vectors.astype('double'))\n",
        "\n",
        "\n",
        "model_02 = KMeans(n_clusters=10, \n",
        "                  max_iter=10000, random_state=None, n_init=100).fit(\n",
        "                      X=trained_model1.vectors.astype('double'))\n",
        "\n",
        "                  \n",
        "positive_cluster_center = model_01.cluster_centers_[0]\n",
        "negative_cluster_center = model_01.cluster_centers_[1]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d73vuex6jSPn"
      },
      "source": [
        "import numpy as np\n",
        "t1 = []\n",
        "t2 = []\n",
        "for i in range(10):\n",
        "  t1.append(trained_model.similar_by_vector(model_01.cluster_centers_[i], topn=1, restrict_vocab=None))\n",
        "  t2.append(trained_model1.similar_by_vector(model_02.cluster_centers_[i], topn=1, restrict_vocab=None))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S7SrtOwofga",
        "outputId": "53cb700b-7ca9-447e-9974-7403e60bc1ea"
      },
      "source": [
        "t1"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('relations', 0.5283156633377075)],\n",
              " [('crucial', 0.5405222177505493)],\n",
              " [('polite', 0.5564737319946289)],\n",
              " [('sense', 0.5592864751815796)],\n",
              " [('equality', 0.5225918292999268)],\n",
              " [('prejudices', 0.4843869209289551)],\n",
              " [('international', 0.46509450674057007)],\n",
              " [('hard', 0.42866256833076477)],\n",
              " [('motives', 0.4929233193397522)],\n",
              " [('umm', 0.505774199962616)]]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcIHOmyhvY6q",
        "outputId": "90a18d4c-c742-4f81-8030-44b4061a525d"
      },
      "source": [
        "t2"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('political', 0.7111340761184692)],\n",
              " [('ulterior', 0.6056719422340393)],\n",
              " [('character', 0.597621500492096)],\n",
              " [('calm', 0.595472514629364)],\n",
              " [('peaceful', 0.5800516605377197)],\n",
              " [('decisions', 0.6671245098114014)],\n",
              " [('care', 0.6884301900863647)],\n",
              " [('hard', 0.6648726463317871)],\n",
              " [('determination', 0.6873366236686707)],\n",
              " [('humility', 0.6237736940383911)]]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nubr5ACu8XdE"
      },
      "source": [
        "t2 = pd.DataFrame(t2)\n",
        "t2.to_csv(\"extracted_words_cbow.txt\", index=0)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gf5I7J_Lw5w"
      },
      "source": [
        "t1 = pd.DataFrame(t1)\n",
        "t1.to_csv(\"extracted_words_sg.txt\", index=0)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3hQoqK7M5uF"
      },
      "source": [],
      "execution_count": 15,
      "outputs": []
    }
  ]
}